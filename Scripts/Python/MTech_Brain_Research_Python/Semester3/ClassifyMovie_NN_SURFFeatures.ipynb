{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing, svm, cross_validation, metrics\n",
    "import math\n",
    "\n",
    "\n",
    "\"\"\"Prepare The Data\"\"\"\n",
    "data = sio.loadmat('/media/hardik/DataPart/4MovieSURFFeatures.mat')\n",
    "X = data['FEATURES']\n",
    "y_ = data['LABELS']\n",
    "\n",
    "# Converting into One Hot Encoding\n",
    "lblBinary = preprocessing.LabelBinarizer()\n",
    "y_ = lblBinary.fit_transform(y_)\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y_, train_size=0.7)\n",
    "\n",
    "\"\"\"\n",
    "Train Data : 12259\n",
    "factors(batch): 1 13 23 41 299 533 943 12259\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def NextBatch(batchSize=41):\n",
    "    data = X_train[NextBatch.batchIndex:NextBatch.batchIndex + batchSize, :], \\\n",
    "           y_train[\n",
    "           NextBatch.batchIndex:NextBatch.batchIndex + batchSize,\n",
    "           :]\n",
    "    NextBatch.batchIndex += batchSize\n",
    "    return data\n",
    "\n",
    "\n",
    "NextBatch.batchIndex = 0\n",
    "\n",
    "\"\"\"End Of Data\"\"\"\n",
    "\n",
    "\"\"\"Create Model\"\"\"\n",
    "\n",
    "# svmModel = svm.SVC()  # kernel='poly',degree=5\n",
    "#\n",
    "# print(y_train.transpose().shape)\n",
    "# svmModel.fit(X_train, y_train.ravel())\n",
    "#\n",
    "# y = svmModel.predict(X_test)\n",
    "\n",
    "learning_rate = 0.01\n",
    "training_epochs = 200\n",
    "display_step = 50\n",
    "batch_size = 41  # 41\n",
    "\n",
    "\n",
    "def NeuralNetwork(dimensions=[64, 50, 25], no_class=4):\n",
    "    \"\"\"Build a deep Neural Network w/ tied weights.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dimensions : list, optional\n",
    "            The number of neurons for each layer of the autoencoder.\n",
    "        number of classes: integer, optional\n",
    "        Returns\n",
    "        -------\n",
    "        X : Tensor\n",
    "            Input placeholder to the network\n",
    "        Y : Tensor\n",
    "            Output softmax probabilities\n",
    "        cost : Tensor\n",
    "            Overall cost to use for training\n",
    "        \"\"\"\n",
    "    X = tf.placeholder(tf.float32, shape=[None, dimensions[0]], name=\"X\")\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, no_class], name=\"Y\")\n",
    "    current_input = X\n",
    "\n",
    "    # Build The classifier\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for layer_i, n_output in enumerate(dimensions[1:]):\n",
    "        n_input = int(current_input.get_shape()[1])\n",
    "        W = tf.Variable(\n",
    "            tf.random_uniform([n_input, n_output],\n",
    "                              -1.0 / math.sqrt(n_input),\n",
    "                              1.0 / math.sqrt(n_input)))\n",
    "        b = tf.Variable(tf.zeros([n_output]))\n",
    "        weights.append(W)\n",
    "        biases.append(b)\n",
    "        output = tf.nn.tanh(tf.matmul(current_input, W) + b)\n",
    "        current_input = output\n",
    "    # Create output Layer\n",
    "    n_input = int(current_input.get_shape()[1])\n",
    "    W = tf.Variable(\n",
    "        tf.random_uniform([n_input, no_class],\n",
    "                          -1.0 / math.sqrt(n_input),\n",
    "                          1.0 / math.sqrt(n_input)))\n",
    "    b = tf.Variable(tf.zeros([no_class]))\n",
    "    weights.append(W)\n",
    "    biases.append(b)\n",
    "    output = tf.nn.sigmoid(tf.matmul(current_input, W) + b)\n",
    "\n",
    "    cost = tf.reduce_sum(tf.square(output - Y))\n",
    "    cross_entropy = -tf.reduce_sum(Y * tf.log(output))\n",
    "    return {'X': X, 'Y': Y, 'cost': cost, 'cross_entropy': cross_entropy,\n",
    "            'model': output, 'W': weights, 'b': biases, 'predict': output}\n",
    "\n",
    "\n",
    "net = NeuralNetwork(dimensions=[64, 50, 20])\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(net['cost'])\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W': [<tensorflow.python.ops.variables.Variable object at 0x7ff952935ac8>, <tensorflow.python.ops.variables.Variable object at 0x7ff92bce4d30>, <tensorflow.python.ops.variables.Variable object at 0x7ff92bce4ef0>], 'b': [<tensorflow.python.ops.variables.Variable object at 0x7ff92bce4cf8>, <tensorflow.python.ops.variables.Variable object at 0x7ff92b854eb8>, <tensorflow.python.ops.variables.Variable object at 0x7ff92b860be0>], 'X': <tf.Tensor 'X:0' shape=(?, 64) dtype=float32>, 'cross_entropy': <tf.Tensor 'Neg:0' shape=() dtype=float32>, 'cost': <tf.Tensor 'Sum:0' shape=() dtype=float32>, 'Y': <tf.Tensor 'Y:0' shape=(?, 4) dtype=float32>, 'model': <tf.Tensor 'Sigmoid:0' shape=(?, 4) dtype=float32>, 'predict': <tf.Tensor 'Sigmoid:0' shape=(?, 4) dtype=float32>}\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 23.986735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 : 14.497884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 : 13.223499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150 : 12.335359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\nAccuracy: 0.773696\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'End of Model Validation'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running Neural Network\n",
    "for ep_i in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(X_train.shape[0] / batch_size)\n",
    "    for i in range(total_batch):\n",
    "        b_x, b_y = NextBatch(batch_size)\n",
    "        _, c = sess.run([optimizer, net['cost']], feed_dict={net['X']: b_x, net['Y']: b_y})\n",
    "        avg_cost += c\n",
    "    avg_cost /= total_batch\n",
    "    NextBatch.batchIndex = 0\n",
    "    if (ep_i % display_step == 0):\n",
    "        print(\"Epoch %d : %f\" % (ep_i, avg_cost))\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "\"\"\"End Of Model Creation\"\"\"\n",
    "\n",
    "\"\"\"Model Validation\"\"\"\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(net['predict'], 1), tf.argmax(y_test, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy:\",\n",
    "      sess.run(accuracy,\n",
    "               feed_dict={net['X']: X_test, net['Y']: y_test}))  # print(metrics.accuracy_score(y_test.ravel(), y))\n",
    "# cm = metrics.confusion_matrix(y_test.ravel(), y)\n",
    "# print(cm)\n",
    "\n",
    "\"\"\"End of Model Validation\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387\n"
     ]
    }
   ],
   "source": [
    "data = sio.loadmat('/media/hardik/DataPart/VisualArea_VideoResponseMapping.mat')\n",
    "features = data.get('FEATURES')\n",
    "mappingLabel = data.get('mappingLabel')\n",
    "\n",
    "n = features.shape[1]\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cost = 0\n",
    "for i in range(n):\n",
    "    batch = features[0, i]\n",
    "    if (batch.shape[0] == 0):\n",
    "        continue\n",
    "    # Get the Result from Model 'Y'\n",
    "    Y = sess.run(net['predict'], feed_dict={net['X']: batch})\n",
    "    ind = mappingLabel[0, i] - 1\n",
    "    Y = Y[:, ind].flatten()\n",
    "    total_cost = total_cost + np.mean(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173.679534449\n"
     ]
    }
   ],
   "source": [
    "FINAL_IMPACT_5 = total_cost #/ 29200  # 29200 is the total time duration of the\n",
    "print(FINAL_IMPACT_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}